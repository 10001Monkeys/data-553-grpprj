{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing workflow\n",
    "## Data 553 Group Project\n",
    "### Chris Donoff, Wei Wei Liu, Bruno Santos, Alex Tamm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.24.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import (absolute_import, division,\n",
    "                        print_function, unicode_literals)\n",
    "\n",
    "import requests #must be installed for tense functions to work\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #for stopword removal\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.parse import CoreNLPParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Our Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appTitle</th>\n",
       "      <th>userName</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>fileDate</th>\n",
       "      <th>fileCategories</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>appId</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label_UE</th>\n",
       "      <th>label_BR</th>\n",
       "      <th>label_FR</th>\n",
       "      <th>label_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>PBS KIDS Video</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>15-Aug-17</td>\n",
       "      <td>5</td>\n",
       "      <td>It,s great gots lot of show it,s great 5  stars</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>org.pbskids.video</td>\n",
       "      <td>1333153</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>IRS2Go</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>23-Mar-15</td>\n",
       "      <td>5</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>gov.irs</td>\n",
       "      <td>1436222</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>Google Play Games</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-Mar-19</td>\n",
       "      <td>1</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Teen</td>\n",
       "      <td>com.google.android.play.games</td>\n",
       "      <td>726513</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855228</td>\n",
       "      <td>Venmo Mobile Wallet: Send &amp; Receive Money</td>\n",
       "      <td>Linda Logan</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>5</td>\n",
       "      <td>great way to send or receive money</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>com.venmo</td>\n",
       "      <td>1414907</td>\n",
       "      <td>great way to send or receive money</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010397</td>\n",
       "      <td>Love Poly - New puzzle game</td>\n",
       "      <td>Valerie P</td>\n",
       "      <td>10-Mar-19</td>\n",
       "      <td>2</td>\n",
       "      <td>the amount of ads is ridiculous.</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>com.love.poly.puzzle.game</td>\n",
       "      <td>1653018</td>\n",
       "      <td>the amount of ads is ridiculous.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                   appTitle       userName  \\\n",
       "0   804084                             PBS KIDS Video  A Google User   \n",
       "1   869576                                     IRS2Go  A Google User   \n",
       "2   425433                          Google Play Games            NaN   \n",
       "3   855228  Venmo Mobile Wallet: Send & Receive Money    Linda Logan   \n",
       "4  1010397                Love Poly - New puzzle game      Valerie P   \n",
       "\n",
       "        date  score                                               text  \\\n",
       "0  15-Aug-17      5    It,s great gots lot of show it,s great 5  stars   \n",
       "1  23-Mar-15      5  Teacher Used the app to check on the status of...   \n",
       "2  19-Mar-19      1         It was enjoyable and educative a good one.   \n",
       "3  05-Mar-19      5                 great way to send or receive money   \n",
       "4  10-Mar-19      2                   the amount of ads is ridiculous.   \n",
       "\n",
       "     fileDate fileCategories contentRating                          appId  \\\n",
       "0  2019-04-14      EDUCATION      Everyone              org.pbskids.video   \n",
       "1  2019-03-24        FINANCE      Everyone                        gov.irs   \n",
       "2  2019-03-24  ENTERTAINMENT          Teen  com.google.android.play.games   \n",
       "3  2019-03-08        FINANCE      Everyone                      com.venmo   \n",
       "4  2019-04-01         FAMILY      Everyone      com.love.poly.puzzle.game   \n",
       "\n",
       "   reviewId                                     processed_text  label_UE  \\\n",
       "0   1333153             It's great, got lots of shows, 5 stars         0   \n",
       "1   1436222  Teacher Used the app to check on the status of...         1   \n",
       "2    726513         It was enjoyable and educative a good one.         0   \n",
       "3   1414907                 great way to send or receive money         0   \n",
       "4   1653018                   the amount of ads is ridiculous.         0   \n",
       "\n",
       "   label_BR  label_FR  label_R  \n",
       "0         0         0        1  \n",
       "1         0         0        0  \n",
       "2         0         0        1  \n",
       "3         0         0        1  \n",
       "4         0         0        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the csv of our data\n",
    "filename = \"sample_384_all_coded.csv\"\n",
    "delimiter = \"\\t\".encode('utf-8')\n",
    "\n",
    "df = pd.read_csv(filename, encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply Stopword Removal to processed_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appTitle</th>\n",
       "      <th>userName</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>fileDate</th>\n",
       "      <th>fileCategories</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>appId</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label_UE</th>\n",
       "      <th>label_BR</th>\n",
       "      <th>label_FR</th>\n",
       "      <th>label_R</th>\n",
       "      <th>stopwords_removal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>PBS KIDS Video</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>15-Aug-17</td>\n",
       "      <td>5</td>\n",
       "      <td>It,s great gots lot of show it,s great 5  stars</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>org.pbskids.video</td>\n",
       "      <td>1333153</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>IRS2Go</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>23-Mar-15</td>\n",
       "      <td>5</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>gov.irs</td>\n",
       "      <td>1436222</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>Google Play Games</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-Mar-19</td>\n",
       "      <td>1</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Teen</td>\n",
       "      <td>com.google.android.play.games</td>\n",
       "      <td>726513</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           appTitle       userName       date  score  \\\n",
       "0  804084     PBS KIDS Video  A Google User  15-Aug-17      5   \n",
       "1  869576             IRS2Go  A Google User  23-Mar-15      5   \n",
       "2  425433  Google Play Games            NaN  19-Mar-19      1   \n",
       "\n",
       "                                                text    fileDate  \\\n",
       "0    It,s great gots lot of show it,s great 5  stars  2019-04-14   \n",
       "1  Teacher Used the app to check on the status of...  2019-03-24   \n",
       "2         It was enjoyable and educative a good one.  2019-03-24   \n",
       "\n",
       "  fileCategories contentRating                          appId  reviewId  \\\n",
       "0      EDUCATION      Everyone              org.pbskids.video   1333153   \n",
       "1        FINANCE      Everyone                        gov.irs   1436222   \n",
       "2  ENTERTAINMENT          Teen  com.google.android.play.games    726513   \n",
       "\n",
       "                                      processed_text  label_UE  label_BR  \\\n",
       "0             It's great, got lots of shows, 5 stars         0         0   \n",
       "1  Teacher Used the app to check on the status of...         1         0   \n",
       "2         It was enjoyable and educative a good one.         0         0   \n",
       "\n",
       "   label_FR  label_R                                  stopwords_removal  \n",
       "0         0        1                It's great, got lots shows, 5 stars  \n",
       "1         0        0  Teacher Used app check status return. Informat...  \n",
       "2         0        1                   It enjoyable educative good one.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df[\"stopwords_removal\"] = df['processed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Lemmatization to processed_text and stopword_removal fields\n",
    "#### (Also count number of words in processed_text field and store in length_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appTitle</th>\n",
       "      <th>userName</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>fileDate</th>\n",
       "      <th>fileCategories</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>appId</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>label_UE</th>\n",
       "      <th>label_BR</th>\n",
       "      <th>label_FR</th>\n",
       "      <th>label_R</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>stopwords_removal_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>PBS KIDS Video</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>15-Aug-17</td>\n",
       "      <td>5</td>\n",
       "      <td>It,s great gots lot of show it,s great 5  stars</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>org.pbskids.video</td>\n",
       "      <td>1333153</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "      <td>it's great, get lot of show , 5 star</td>\n",
       "      <td>it's great, get lot show , 5 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>IRS2Go</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>23-Mar-15</td>\n",
       "      <td>5</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>gov.irs</td>\n",
       "      <td>1436222</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "      <td>teacher use the app to check on the status of ...</td>\n",
       "      <td>teacher use app check status return . informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>Google Play Games</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-Mar-19</td>\n",
       "      <td>1</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Teen</td>\n",
       "      <td>com.google.android.play.games</td>\n",
       "      <td>726513</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "      <td>it be enjoyable and educative a good one.</td>\n",
       "      <td>it enjoyable educative good one.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           appTitle       userName       date  score  \\\n",
       "0  804084     PBS KIDS Video  A Google User  15-Aug-17      5   \n",
       "1  869576             IRS2Go  A Google User  23-Mar-15      5   \n",
       "2  425433  Google Play Games            NaN  19-Mar-19      1   \n",
       "\n",
       "                                                text    fileDate  \\\n",
       "0    It,s great gots lot of show it,s great 5  stars  2019-04-14   \n",
       "1  Teacher Used the app to check on the status of...  2019-03-24   \n",
       "2         It was enjoyable and educative a good one.  2019-03-24   \n",
       "\n",
       "  fileCategories contentRating                          appId  reviewId  \\\n",
       "0      EDUCATION      Everyone              org.pbskids.video   1333153   \n",
       "1        FINANCE      Everyone                        gov.irs   1436222   \n",
       "2  ENTERTAINMENT          Teen  com.google.android.play.games    726513   \n",
       "\n",
       "                                      processed_text  label_UE  label_BR  \\\n",
       "0             It's great, got lots of shows, 5 stars         0         0   \n",
       "1  Teacher Used the app to check on the status of...         1         0   \n",
       "2         It was enjoyable and educative a good one.         0         0   \n",
       "\n",
       "   label_FR  label_R                                  stopwords_removal  \\\n",
       "0         0        1                It's great, got lots shows, 5 stars   \n",
       "1         0        0  Teacher Used app check status return. Informat...   \n",
       "2         0        1                   It enjoyable educative good one.   \n",
       "\n",
       "                                  lemmatized_comment  \\\n",
       "0               it's great, get lot of show , 5 star   \n",
       "1  teacher use the app to check on the status of ...   \n",
       "2          it be enjoyable and educative a good one.   \n",
       "\n",
       "                        stopwords_removal_lemmatized  \n",
       "0                  it's great, get lot show , 5 star  \n",
       "1  teacher use app check status return . informat...  \n",
       "2                   it enjoyable educative good one.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply lemmatization to df (create lemmatized_comment and stopwords_removal_lemmatized and length of words)\n",
    "wn_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "temp=[]\n",
    "for i in df.processed_text:\n",
    "    ##tokenize\n",
    "    title_token_list = nltk.word_tokenize(i)\n",
    "    ##lowercase\n",
    "    title_token_list = [word.lower() for word in title_token_list]\n",
    "    ##lemmatize verb.\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w,pos='v') for w in title_token_list]\n",
    "    ##lemmatize noun\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w) for w in title_token_list]\n",
    "    ##lemmatize adj.\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w,pos='a') for w in title_token_list]\n",
    "    ##detokenize\n",
    "    title_token_list= TreebankWordDetokenizer().detokenize(title_token_list)\n",
    "    temp.append(title_token_list)\n",
    "\n",
    "\n",
    "df['lemmatized_comment']=temp\n",
    "\n",
    "temp = []\n",
    "for i in df.stopwords_removal:\n",
    "    ##tokenize\n",
    "    title_token_list = nltk.word_tokenize(i)\n",
    "    ##lowercase\n",
    "    title_token_list = [word.lower() for word in title_token_list]\n",
    "    ##lemmatize verb.\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w,pos='v') for w in title_token_list]\n",
    "    ##lemmatize noun\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w) for w in title_token_list]\n",
    "    ##lemmatize adj.\n",
    "    title_token_list = [wn_lemmatizer.lemmatize(w,pos='a') for w in title_token_list]\n",
    "    ##detokenize\n",
    "    title_token_list= TreebankWordDetokenizer().detokenize(title_token_list)\n",
    "    temp.append(title_token_list)\n",
    "df['stopwords_removal_lemmatized']=temp\n",
    "\n",
    "wlen=None\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply Tense to processed_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22ssplit.isOneSentence%22%3A+%22true%22%2C+%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000000109C6888>: Failed to establish a new connection: [Errno 10061] No connection could be made because the target machine actively refused it',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-893ea7e5f1d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpos_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://localhost:9000'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtagged_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mfuture_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    366\u001b[0m         ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mraw_tag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Converting list(list(str)) -> list(str)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_tag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mraw_tag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mdefault_properties\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'annotators'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mtagged_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m             yield [\n\u001b[0;32m    391\u001b[0m                 [\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mapi_call\u001b[1;34m(self, data, properties, timeout)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'properties'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         )\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \"\"\"\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22ssplit.isOneSentence%22%3A+%22true%22%2C+%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000000109C6888>: Failed to establish a new connection: [Errno 10061] No connection could be made because the target machine actively refused it',))"
     ]
    }
   ],
   "source": [
    "#This script counts the following types of tense and compares to values in Bug_tt:\n",
    "#VBG = present_cont\n",
    "#VB and VBZ= present_simple\n",
    "#VBD and VBN = past tense\n",
    "#future: MD = 'will' 'shall'\n",
    "\n",
    "#the following script can be modified to read the processed_text column coming from our 542 data frame, and\n",
    "#then add these counts as 4 new columns to the df. These 4 columns are in replic_tense\n",
    "\n",
    "\n",
    "future = []\n",
    "past = []\n",
    "present_simple = []\n",
    "present_con = []\n",
    "\n",
    "for i in np.arange(len(df['processed_text'])):\n",
    "    pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
    "    tagged_words = list(pos_tagger.tag(df['processed_text'][i].split()))\n",
    "\n",
    "    future_count = 0\n",
    "    past_count = 0\n",
    "    present_simple_count = 0\n",
    "    present_con_count = 0\n",
    "    for i in np.arange(len(tagged_words)):\n",
    "        if tagged_words[i][1] == 'MD' and (tagged_words[i][0] == 'will' or tagged_words[i][0] == 'shall'):\n",
    "            future_count+= 1\n",
    "        if (tagged_words[i][1] == 'VBD' or tagged_words[i][1] == 'VBN'):\n",
    "            past_count+= 1\n",
    "        if (tagged_words[i][1] == 'VBP' or tagged_words[i][1] == 'VBZ' or tagged_words[i][1] == 'VB'):\n",
    "            present_simple_count+= 1\n",
    "        if tagged_words[i][1] == 'VBG':\n",
    "            present_con_count+= 1\n",
    "            \n",
    "    future.append(future_count)\n",
    "    past.append(past_count)\n",
    "    present_simple.append(present_simple_count)\n",
    "    present_con.append(present_con_count)\n",
    "\n",
    "tense_dict = {'future': future, 'past': past, 'present_simple': present_simple, 'present_con': present_con}     \n",
    "replic_tense = pd.DataFrame(tense_dict)\n",
    "replic_tense = replic_tense[['future', 'past', 'present_simple','present_con']]\n",
    "\n",
    "df[['future','past', 'present_simple','present_con']] = replic_tense[['future','past','present_simple','present_con']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Sentiment (step 1/3): extract data to feed into standalone SentiScore software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_384_all_coded.csv_for_senti.txt saved for input to sentiStrength\n"
     ]
    }
   ],
   "source": [
    "#Apply sentiment to df (create sentiScore sentiScore_pos sentiScore_neg)\n",
    "tempDF = df[[\"id\",\"processed_text\"]].copy()\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\n\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\r\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\t\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.rstrip() #white space at the end of a processed_text was creating a new line upon export\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.lstrip() #no reason to keep leading whitespaces, so stripping off\n",
    "tempDF[[\"id\",\"processed_text\"]].to_csv(filename+\"_for_senti.txt\", header=True, index=False, sep=delimiter, encoding = \"utf-8\")\n",
    "print(filename+\"_for_senti.txt saved for input to sentiStrength\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Sentiment (step 2/3): use standalone SentiScore sofware with the following parameters/instructions:\n",
    "\n",
    "(**This step must be performed manually outside of the notebook BUT the result file from the previous time this process was done is stored in the folder already so you can skip this step)**\n",
    "\n",
    "Use SentiStrength 2.3 from (http://sentistrength.wlv.ac.uk/)\n",
    "Use Sept 21, 2011 configuration files downloaded from same site\n",
    "Use the following settings in menu:\n",
    "![settings0](sshot0-senti_settings0.png)\n",
    "![settings1](sshot1-senti_settings1.png)\n",
    "![settings1](sshot2-senti_settings2.png)\n",
    "![settings1](sshot3-senti_settings3.png)\n",
    "\n",
    "- Use Sentiment Strength Analysis -> Analyze ALL Texts in File....As Above for ALL files in folder\n",
    "\n",
    "- Select the file that was exported from the previous step (\"sample_384_all_coded.csv_for_senti.txt\")\n",
    "\n",
    "- \"Yes\" when prompted if it should echo the header in the results\n",
    "\n",
    "- When prompted which column contains text, enter \"2\" (do not use the default value of 3)\n",
    "\n",
    "- Results will be saved with \"+results\" appended to the filename in the folder where the input file(s) were stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Sentiment (step 3/3): read results back in and add to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>appTitle</th>\n",
       "      <th>userName</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>fileDate</th>\n",
       "      <th>fileCategories</th>\n",
       "      <th>contentRating</th>\n",
       "      <th>appId</th>\n",
       "      <th>...</th>\n",
       "      <th>label_UE</th>\n",
       "      <th>label_BR</th>\n",
       "      <th>label_FR</th>\n",
       "      <th>label_R</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>stopwords_removal_lemmatized</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>sentiScore_neg</th>\n",
       "      <th>sentiScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>PBS KIDS Video</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>15-Aug-17</td>\n",
       "      <td>5</td>\n",
       "      <td>It,s great gots lot of show it,s great 5  stars</td>\n",
       "      <td>2019-04-14</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>org.pbskids.video</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "      <td>it's great, get lot of show , 5 star</td>\n",
       "      <td>it's great, get lot show , 5 star</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>IRS2Go</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>23-Mar-15</td>\n",
       "      <td>5</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>gov.irs</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "      <td>teacher use the app to check on the status of ...</td>\n",
       "      <td>teacher use app check status return . informat...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>Google Play Games</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-Mar-19</td>\n",
       "      <td>1</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Teen</td>\n",
       "      <td>com.google.android.play.games</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "      <td>it be enjoyable and educative a good one.</td>\n",
       "      <td>it enjoyable educative good one.</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855228</td>\n",
       "      <td>Venmo Mobile Wallet: Send &amp; Receive Money</td>\n",
       "      <td>Linda Logan</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>5</td>\n",
       "      <td>great way to send or receive money</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>com.venmo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>great way send receive money</td>\n",
       "      <td>great way to send or receive money</td>\n",
       "      <td>great way send receive money</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010397</td>\n",
       "      <td>Love Poly - New puzzle game</td>\n",
       "      <td>Valerie P</td>\n",
       "      <td>10-Mar-19</td>\n",
       "      <td>2</td>\n",
       "      <td>the amount of ads is ridiculous.</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>FAMILY</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>com.love.poly.puzzle.game</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>amount ads ridiculous.</td>\n",
       "      <td>the amount of ad be ridiculous.</td>\n",
       "      <td>amount ad ridiculous.</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                   appTitle       userName  \\\n",
       "0   804084                             PBS KIDS Video  A Google User   \n",
       "1   869576                                     IRS2Go  A Google User   \n",
       "2   425433                          Google Play Games            NaN   \n",
       "3   855228  Venmo Mobile Wallet: Send & Receive Money    Linda Logan   \n",
       "4  1010397                Love Poly - New puzzle game      Valerie P   \n",
       "\n",
       "        date  score                                               text  \\\n",
       "0  15-Aug-17      5    It,s great gots lot of show it,s great 5  stars   \n",
       "1  23-Mar-15      5  Teacher Used the app to check on the status of...   \n",
       "2  19-Mar-19      1         It was enjoyable and educative a good one.   \n",
       "3  05-Mar-19      5                 great way to send or receive money   \n",
       "4  10-Mar-19      2                   the amount of ads is ridiculous.   \n",
       "\n",
       "     fileDate fileCategories contentRating                          appId  \\\n",
       "0  2019-04-14      EDUCATION      Everyone              org.pbskids.video   \n",
       "1  2019-03-24        FINANCE      Everyone                        gov.irs   \n",
       "2  2019-03-24  ENTERTAINMENT          Teen  com.google.android.play.games   \n",
       "3  2019-03-08        FINANCE      Everyone                      com.venmo   \n",
       "4  2019-04-01         FAMILY      Everyone      com.love.poly.puzzle.game   \n",
       "\n",
       "   ...  label_UE label_BR  label_FR  label_R  \\\n",
       "0  ...         0        0         0        1   \n",
       "1  ...         1        0         0        0   \n",
       "2  ...         0        0         0        1   \n",
       "3  ...         0        0         0        1   \n",
       "4  ...         0        0         0        1   \n",
       "\n",
       "                                   stopwords_removal  \\\n",
       "0                It's great, got lots shows, 5 stars   \n",
       "1  Teacher Used app check status return. Informat...   \n",
       "2                   It enjoyable educative good one.   \n",
       "3                       great way send receive money   \n",
       "4                             amount ads ridiculous.   \n",
       "\n",
       "                                  lemmatized_comment  \\\n",
       "0               it's great, get lot of show , 5 star   \n",
       "1  teacher use the app to check on the status of ...   \n",
       "2          it be enjoyable and educative a good one.   \n",
       "3                 great way to send or receive money   \n",
       "4                    the amount of ad be ridiculous.   \n",
       "\n",
       "                        stopwords_removal_lemmatized sentiScore_pos  \\\n",
       "0                  it's great, get lot show , 5 star              3   \n",
       "1  teacher use app check status return . informat...              1   \n",
       "2                   it enjoyable educative good one.              3   \n",
       "3                       great way send receive money              3   \n",
       "4                              amount ad ridiculous.              1   \n",
       "\n",
       "  sentiScore_neg  sentiScore  \n",
       "0             -1           3  \n",
       "1             -1          -1  \n",
       "2             -1           3  \n",
       "3             -1           3  \n",
       "4             -3          -3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates Sentiscore given SentiScore_pos and SentiScore_neg ratings (picks the value that is furthest from 0)\n",
    "def find_max_sent(vect):\n",
    "    pos=vect[0]\n",
    "    neg=vect[1]\n",
    "    if abs(pos)>abs(neg):\n",
    "        return pos\n",
    "    else:\n",
    "        return neg #The authors of the paper appear to have used the negative score in the event of a tie\n",
    "\n",
    "\n",
    "\n",
    "resultsDF = pd.read_csv(filename+\"_for_senti+results.txt\", sep=delimiter, encoding=\"utf-8\")\n",
    "df = df.merge(resultsDF[[\"id\",\"Positive\",\"Negative\"]], on=[\"id\"])\n",
    "\n",
    "#use find_max_sent() function to assign a single sentiment score \n",
    "df[\"sentiScore\"] = df[[\"Positive\",\"Negative\"]].apply(find_max_sent, axis=1)\n",
    "#rename the other two sentiment score columns to match what they are called in the paper's original dataset\n",
    "df.rename(columns={\"Positive\":\"sentiScore_pos\",\"Negative\":\"sentiScore_neg\"}, inplace=True)\n",
    "df.groupby([\"sentiScore\"]).id.count()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>appId</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>comment</th>\n",
       "      <th>label_UE</th>\n",
       "      <th>label_BR</th>\n",
       "      <th>label_FR</th>\n",
       "      <th>...</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>stopwords_removal_lemmatized</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>sentiScore_neg</th>\n",
       "      <th>sentiScore</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>fee</th>\n",
       "      <th>title</th>\n",
       "      <th>dataSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>15-Aug-17</td>\n",
       "      <td>5</td>\n",
       "      <td>org.pbskids.video</td>\n",
       "      <td>1333153</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "      <td>it's great, get lot of show , 5 star</td>\n",
       "      <td>it's great, get lot show , 5 star</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data542_Dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>A Google User</td>\n",
       "      <td>23-Mar-15</td>\n",
       "      <td>5</td>\n",
       "      <td>gov.irs</td>\n",
       "      <td>1436222</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "      <td>teacher use the app to check on the status of ...</td>\n",
       "      <td>teacher use app check status return . informat...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data542_Dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19-Mar-19</td>\n",
       "      <td>1</td>\n",
       "      <td>com.google.android.play.games</td>\n",
       "      <td>726513</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "      <td>it be enjoyable and educative a good one.</td>\n",
       "      <td>it enjoyable educative good one.</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Data542_Dataset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       reviewer       date  rating                          appId  \\\n",
       "0  804084  A Google User  15-Aug-17       5              org.pbskids.video   \n",
       "1  869576  A Google User  23-Mar-15       5                        gov.irs   \n",
       "2  425433            NaN  19-Mar-19       1  com.google.android.play.games   \n",
       "\n",
       "   reviewId                                            comment  label_UE  \\\n",
       "0   1333153             It's great, got lots of shows, 5 stars         0   \n",
       "1   1436222  Teacher Used the app to check on the status of...         1   \n",
       "2    726513         It was enjoyable and educative a good one.         0   \n",
       "\n",
       "   label_BR  label_FR  ...                                  stopwords_removal  \\\n",
       "0         0         0  ...                It's great, got lots shows, 5 stars   \n",
       "1         0         0  ...  Teacher Used app check status return. Informat...   \n",
       "2         0         0  ...                   It enjoyable educative good one.   \n",
       "\n",
       "                                  lemmatized_comment  \\\n",
       "0               it's great, get lot of show , 5 star   \n",
       "1  teacher use the app to check on the status of ...   \n",
       "2          it be enjoyable and educative a good one.   \n",
       "\n",
       "                        stopwords_removal_lemmatized sentiScore_pos  \\\n",
       "0                  it's great, get lot show , 5 star              3   \n",
       "1  teacher use app check status return . informat...              1   \n",
       "2                   it enjoyable educative good one.              3   \n",
       "\n",
       "   sentiScore_neg  sentiScore  stemmed  fee  title       dataSource  \n",
       "0              -1           3      NaN  NaN    NaN  Data542_Dataset  \n",
       "1              -1          -1      NaN  NaN    NaN  Data542_Dataset  \n",
       "2              -1           3      NaN  NaN    NaN  Data542_Dataset  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rename add/remove to match bug_tt.json\n",
    "\n",
    "df.rename(columns={\"processed_text\":\"comment\",\"score\":\"rating\",\"userName\":\"reviewer\"}, inplace=True)\n",
    "\n",
    "df.drop(columns=[\"appTitle\",\"text\",\"fileDate\",\"fileCategories\",\"contentRating\"], inplace=True)\n",
    "\n",
    "df['stemmed'] = np.nan\n",
    "df['fee'] = np.nan\n",
    "df['title'] = np.nan\n",
    "df['dataSource'] = \"Data542_Dataset\"\n",
    "\n",
    "df.head(3)\n",
    "# Use ourAwesomeDF.rename(columns={\"original column name\":\"new column name\", \"another column name\":\"new column name for it\"}, inplace=True) \n",
    "# to rename all columns to match what they are called in bugs_tt.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: UserExperience\n",
      "Number of UserExperience reviews available: 39\n",
      "Number of NOT_UserExperience reviews available: 345\n",
      "Selecting first 39 NOT_UserExperience reviews.\n",
      "\n",
      "Processing: Bug\n",
      "Number of Bug reviews available: 58\n",
      "Number of NOT_Bug reviews available: 326\n",
      "Selecting first 58 NOT_Bug reviews.\n",
      "\n",
      "Processing: Feature\n",
      "Number of Feature reviews available: 43\n",
      "Number of NOT_Feature reviews available: 341\n",
      "Selecting first 43 NOT_Feature reviews.\n",
      "\n",
      "Processing: Rating\n",
      "Number of Rating reviews available: 319\n",
      "Number of NOT_Rating reviews available: 65\n",
      "**Only 65 NOT_Rating reviews are available. Will use all of them and...\n",
      "Removing 254 Rating reviews to create a balanced data set.\n"
     ]
    }
   ],
   "source": [
    "#Output as .json\n",
    "\n",
    "colList = [\"label_UE\",\"label_BR\",\"label_FR\",\"label_R\"]\n",
    "labelNames = [\"UserExperience\",\"Bug\",\"Feature\",\"Rating\"]\n",
    "for n in range(0,len(colList)):\n",
    "    print(\"\\nProcessing:\",labelNames[n])\n",
    "    \n",
    "    #Make a temp dataframe with all of the reviews of the specific label\n",
    "    tempdf=df.drop(colList, axis=1)[df[colList[n]]==1]\n",
    "    tempdf[\"label\"] = labelNames[n]\n",
    "    cnt = len(tempdf) #how many reviews are there of the specific label\n",
    "    print(\"Number of\",labelNames[n],\"reviews available:\",cnt)\n",
    "    \n",
    "    #Make a second temp dataframe with all of the reviews that are NOT the specific label\n",
    "    notTempdf=df.drop(colList, axis=1)[df[colList[n]]==0]\n",
    "    notTempdf[\"label\"] = (\"Not_\"+labelNames[n])\n",
    "    #Select only as many as are needed to match the amount of reviews that are for the label\n",
    "    cnt2 = len(notTempdf)\n",
    "    print(\"Number of NOT_\"+labelNames[n],\"reviews available:\",cnt2)\n",
    "    if cnt2 >= cnt:\n",
    "        print(\"Selecting first\",cnt,\"NOT_\"+labelNames[n],\"reviews.\")\n",
    "        notTempdf=notTempdf.iloc[0:cnt].copy()\n",
    "    else:\n",
    "        print(\"**Only\", cnt2, \"NOT_\"+labelNames[n], \"reviews are available. Will use all of them and...\")\n",
    "        print(\"Removing\",cnt-cnt2,labelNames[n],\"reviews to create a balanced data set.\")\n",
    "        tempdf = tempdf.iloc[0:cnt2].copy()\n",
    "\n",
    "    tempdf = tempdf.append(notTempdf)\n",
    "    tempdf.to_json(labelNames[n]+\"_ourdata.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
