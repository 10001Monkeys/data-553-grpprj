{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing workflow\n",
    "## Data 553 Group Project\n",
    "### Chris Donoff, Wei Wei Liu, Bruno Santos, Alex Tamm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.24.2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import (absolute_import, division,\n",
    "                        print_function, unicode_literals)\n",
    "\n",
    "import requests #must be installed for tense functions to work\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #for stopword removal\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.parse import CoreNLPParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Our Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 384 entries, 0 to 383\n",
      "Data columns (total 16 columns):\n",
      "id                384 non-null int64\n",
      "appTitle          384 non-null object\n",
      "userName          354 non-null object\n",
      "date              384 non-null object\n",
      "score             384 non-null int64\n",
      "text              384 non-null object\n",
      "fileDate          384 non-null object\n",
      "fileCategories    384 non-null object\n",
      "contentRating     384 non-null object\n",
      "appId             384 non-null object\n",
      "reviewId          384 non-null int64\n",
      "processed_text    384 non-null object\n",
      "label_UE          384 non-null int64\n",
      "label_BR          384 non-null int64\n",
      "label_FR          384 non-null int64\n",
      "label_R           384 non-null int64\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 48.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#Load the csv of our data\n",
    "filename = \"sample_384_all_coded.csv\"\n",
    "delimiter = \"\\t\".encode('utf-8')\n",
    "\n",
    "df = pd.read_csv(filename, encoding=\"utf-8\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Stopword Removal to processed_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>stopwords_removal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                     processed_text  \\\n",
       "0  804084             It's great, got lots of shows, 5 stars   \n",
       "1  869576  Teacher Used the app to check on the status of...   \n",
       "2  425433         It was enjoyable and educative a good one.   \n",
       "\n",
       "                                   stopwords_removal  \n",
       "0                It's great, got lots shows, 5 stars  \n",
       "1  Teacher Used app check status return. Informat...  \n",
       "2                   It enjoyable educative good one.  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "df[\"stopwords_removal\"] = df['processed_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "df[[\"id\",\"processed_text\",\"stopwords_removal\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Lemmatization to processed_text and stopword_removal fields\n",
    "### (Also count number of words in processed_text field and store in length_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length_words</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>stopwords_removal_lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>11</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>it's great, get lot of show , 5 star</td>\n",
       "      <td>It's great, got lots shows, 5 stars</td>\n",
       "      <td>it's great, get lot show , 5 star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>30</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>teacher use the app to check on the status of ...</td>\n",
       "      <td>Teacher Used app check status return. Informat...</td>\n",
       "      <td>teacher use app check status return . informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>9</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>it be enjoyable and educative a good one.</td>\n",
       "      <td>It enjoyable educative good one.</td>\n",
       "      <td>it enjoyable educative good one.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  length_words                                     processed_text  \\\n",
       "0  804084            11             It's great, got lots of shows, 5 stars   \n",
       "1  869576            30  Teacher Used the app to check on the status of...   \n",
       "2  425433             9         It was enjoyable and educative a good one.   \n",
       "\n",
       "                                  lemmatized_comment  \\\n",
       "0               it's great, get lot of show , 5 star   \n",
       "1  teacher use the app to check on the status of ...   \n",
       "2          it be enjoyable and educative a good one.   \n",
       "\n",
       "                                   stopwords_removal  \\\n",
       "0                It's great, got lots shows, 5 stars   \n",
       "1  Teacher Used app check status return. Informat...   \n",
       "2                   It enjoyable educative good one.   \n",
       "\n",
       "                     stopwords_removal_lemmatization  \n",
       "0                  it's great, get lot show , 5 star  \n",
       "1  teacher use app check status return . informat...  \n",
       "2                   it enjoyable educative good one.  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "temp=[]\n",
    "length=[]\n",
    "for i in df.processed_text:\n",
    "    ##tokenize\n",
    "    title_token_list = nltk.word_tokenize(i)\n",
    "    length.append(len(title_token_list))\n",
    "    new=[]\n",
    "    for word in title_token_list:\n",
    "        ##lowercase\n",
    "        word = word.lower()\n",
    "        ##lemmatize verb.\n",
    "        word = wn_lemmatizer.lemmatize(word,pos='v')\n",
    "        ##lemmatize noun\n",
    "        word = wn_lemmatizer.lemmatize(word) \n",
    "        ##lemmatize adj.\n",
    "        word = wn_lemmatizer.lemmatize(word,pos='a')\n",
    "        new.append(word)\n",
    "    ##detokenize\n",
    "    title_token_list= TreebankWordDetokenizer().detokenize(new)\n",
    "    temp.append(title_token_list)\n",
    "    \n",
    "df['lemmatized_comment']=temp\n",
    "df['length_words']=length\n",
    "\n",
    "temp = []\n",
    "for i in df.stopwords_removal:\n",
    "    ##tokenize\n",
    "    title_token_list = nltk.word_tokenize(i)\n",
    "    new=[]\n",
    "    for word in title_token_list:\n",
    "        ##lowercase\n",
    "        word = word.lower()\n",
    "        ##lemmatize verb.\n",
    "        word = wn_lemmatizer.lemmatize(word,pos='v')\n",
    "        ##lemmatize noun\n",
    "        word = wn_lemmatizer.lemmatize(word) \n",
    "        ##lemmatize adj.\n",
    "        word = wn_lemmatizer.lemmatize(word,pos='a')\n",
    "        new.append(word)\n",
    "    ##detokenize\n",
    "    title_token_list= TreebankWordDetokenizer().detokenize(new)\n",
    "    temp.append(title_token_list)\n",
    "df['stopwords_removal_lemmatization']=temp\n",
    "\n",
    "df[[\"id\",\"length_words\",\"processed_text\",\"lemmatized_comment\",\"stopwords_removal\",\"stopwords_removal_lemmatization\"]].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Tense to processed_text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22ssplit.isOneSentence%22%3A+%22true%22%2C+%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000000010D9A288>: Failed to establish a new connection: [Errno 10061] No connection could be made because the target machine actively refused it',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-893ea7e5f1d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpos_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoreNLPParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://localhost:9000'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtagged_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'processed_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mfuture_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    366\u001b[0m         ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n\u001b[0;32m    367\u001b[0m         \"\"\"\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mraw_tag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Converting list(list(str)) -> list(str)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_tag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mraw_tag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mdefault_properties\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'annotators'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mtagged_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m             yield [\n\u001b[0;32m    391\u001b[0m                 [\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\nltk\\parse\\corenlp.pyc\u001b[0m in \u001b[0;36mapi_call\u001b[1;34m(self, data, properties, timeout)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'properties'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         )\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \"\"\"\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex\\Anaconda3\\envs\\easy\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22ssplit.isOneSentence%22%3A+%22true%22%2C+%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cssplit%2Cpos%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000000010D9A288>: Failed to establish a new connection: [Errno 10061] No connection could be made because the target machine actively refused it',))"
     ]
    }
   ],
   "source": [
    "#This script counts the following types of tense and compares to values in Bug_tt:\n",
    "#VBG = present_cont\n",
    "#VB and VBZ= present_simple\n",
    "#VBD and VBN = past tense\n",
    "#future: MD = 'will' 'shall'\n",
    "\n",
    "#the following script can be modified to read the processed_text column coming from our 542 data frame, and\n",
    "#then add these counts as 4 new columns to the df. These 4 columns are in replic_tense\n",
    "\n",
    "\n",
    "future = []\n",
    "past = []\n",
    "present_simple = []\n",
    "present_con = []\n",
    "\n",
    "for i in np.arange(len(df['processed_text'])):\n",
    "    pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
    "    tagged_words = list(pos_tagger.tag(df['processed_text'][i].split()))\n",
    "\n",
    "    future_count = 0\n",
    "    past_count = 0\n",
    "    present_simple_count = 0\n",
    "    present_con_count = 0\n",
    "    for i in np.arange(len(tagged_words)):\n",
    "        if tagged_words[i][1] == 'MD' and (tagged_words[i][0] == 'will' or tagged_words[i][0] == 'shall'):\n",
    "            future_count+= 1\n",
    "        if (tagged_words[i][1] == 'VBD' or tagged_words[i][1] == 'VBN'):\n",
    "            past_count+= 1\n",
    "        if (tagged_words[i][1] == 'VBP' or tagged_words[i][1] == 'VBZ' or tagged_words[i][1] == 'VB'):\n",
    "            present_simple_count+= 1\n",
    "        if tagged_words[i][1] == 'VBG':\n",
    "            present_con_count+= 1\n",
    "            \n",
    "    future.append(future_count)\n",
    "    past.append(past_count)\n",
    "    present_simple.append(present_simple_count)\n",
    "    present_con.append(present_con_count)\n",
    "\n",
    "tense_dict = {'future': future, 'past': past, 'present_simple': present_simple, 'present_con': present_con}     \n",
    "replic_tense = pd.DataFrame(tense_dict)\n",
    "replic_tense = replic_tense[['future', 'past', 'present_simple','present_con']]\n",
    "\n",
    "df[['future','past', 'present_simple','present_con']] = replic_tense[['future','past','present_simple','present_con']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Sentiment (step 1/3): \n",
    "### extract data to feed into standalone SentiScore software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_384_all_coded.csv_for_senti.txt saved for input to sentiStrength\n"
     ]
    }
   ],
   "source": [
    "#Apply sentiment to df (create sentiScore sentiScore_pos sentiScore_neg)\n",
    "tempDF = df[[\"id\",\"processed_text\"]].copy()\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\n\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\r\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.replace(\"\\t\",\" \",regex=False)\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.rstrip() #white space at the end of a processed_text was creating a new line upon export\n",
    "tempDF[\"processed_text\"]=tempDF[\"processed_text\"].str.lstrip() #no reason to keep leading whitespaces, so stripping off\n",
    "tempDF[[\"id\",\"processed_text\"]].to_csv(filename+\"_for_senti.txt\", header=True, index=False, sep=delimiter, encoding = \"utf-8\")\n",
    "print(filename+\"_for_senti.txt saved for input to sentiStrength\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Sentiment (step 2/3): \n",
    "### use standalone SentiScore sofware with the following parameters/instructions:\n",
    "\n",
    "(**This step must be performed manually outside of the notebook BUT the result file from the previous time this process was done is stored in the folder already so you can skip this step)**\n",
    "\n",
    "Use SentiStrength 2.3 from (http://sentistrength.wlv.ac.uk/)\n",
    "Use Sept 21, 2011 configuration files downloaded from same site\n",
    "Use the following settings in menu:\n",
    "![settings0](sshot0-senti_settings0.png)\n",
    "![settings1](sshot1-senti_settings1.png)\n",
    "![settings1](sshot2-senti_settings2.png)\n",
    "![settings1](sshot3-senti_settings3.png)\n",
    "\n",
    "- Use Sentiment Strength Analysis -> Analyze ALL Texts in File....As Above for ALL files in folder\n",
    "\n",
    "- Select the file that was exported from the previous step (\"sample_384_all_coded.csv_for_senti.txt\")\n",
    "\n",
    "- \"Yes\" when prompted if it should echo the header in the results\n",
    "\n",
    "- When prompted which column contains text, enter \"2\" (do not use the default value of 3)\n",
    "\n",
    "- Results will be saved with \"+results\" appended to the filename in the folder where the input file(s) were stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c. Sentiment (step 3/3): read results back in and add to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sentiScore</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>sentiScore_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804084</td>\n",
       "      <td>It's great, got lots of shows, 5 stars</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869576</td>\n",
       "      <td>Teacher Used the app to check on the status of...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425433</td>\n",
       "      <td>It was enjoyable and educative a good one.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855228</td>\n",
       "      <td>great way to send or receive money</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010397</td>\n",
       "      <td>the amount of ads is ridiculous.</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                     processed_text  sentiScore  \\\n",
       "0   804084             It's great, got lots of shows, 5 stars           3   \n",
       "1   869576  Teacher Used the app to check on the status of...          -1   \n",
       "2   425433         It was enjoyable and educative a good one.           3   \n",
       "3   855228                 great way to send or receive money           3   \n",
       "4  1010397                   the amount of ads is ridiculous.          -3   \n",
       "\n",
       "   sentiScore_pos  sentiScore_neg  \n",
       "0               3              -1  \n",
       "1               1              -1  \n",
       "2               3              -1  \n",
       "3               3              -1  \n",
       "4               1              -3  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculates Sentiscore given SentiScore_pos and SentiScore_neg ratings (picks the value that is furthest from 0)\n",
    "def find_max_sent(vect):\n",
    "    pos=vect[0]\n",
    "    neg=vect[1]\n",
    "    if abs(pos)>abs(neg):\n",
    "        return pos\n",
    "    else:\n",
    "        return neg #The authors of the paper appear to have used the negative score in the event of a tie\n",
    "\n",
    "\n",
    "\n",
    "resultsDF = pd.read_csv(filename+\"_for_senti+results.txt\", sep=delimiter, encoding=\"utf-8\")\n",
    "resultsDF.rename(columns={\"Positive\":\"sentiScore_pos\",\"Negative\":\"sentiScore_neg\"}, inplace=True)\n",
    "df = df.merge(resultsDF[[\"id\",\"sentiScore_pos\",\"sentiScore_neg\"]], on=[\"id\"])\n",
    "\n",
    "#use find_max_sent() function to assign a single sentiment score \n",
    "df[\"sentiScore\"] = df[[\"sentiScore_pos\",\"sentiScore_neg\"]].apply(find_max_sent, axis=1)\n",
    "#rename the other two sentiment score columns to match what they are called in the paper's original dataset\n",
    "df.groupby([\"sentiScore\"]).id.count()\n",
    "df[[\"id\",\"processed_text\",\"sentiScore\",\"sentiScore_pos\",\"sentiScore_neg\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 6. Rename and and/drop columns \n",
    "### (to match the structure of the input files used in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 384 entries, 0 to 383\n",
      "Data columns (total 22 columns):\n",
      "id                                 384 non-null int64\n",
      "reviewer                           354 non-null object\n",
      "date                               384 non-null object\n",
      "rating                             384 non-null int64\n",
      "appId                              384 non-null object\n",
      "reviewId                           384 non-null int64\n",
      "comment                            384 non-null object\n",
      "label_UE                           384 non-null int64\n",
      "label_BR                           384 non-null int64\n",
      "label_FR                           384 non-null int64\n",
      "label_R                            384 non-null int64\n",
      "stopwords_removal                  384 non-null object\n",
      "lemmatized_comment                 384 non-null object\n",
      "length_words                       384 non-null int64\n",
      "stopwords_removal_lemmatization    384 non-null object\n",
      "sentiScore_pos                     384 non-null int64\n",
      "sentiScore_neg                     384 non-null int64\n",
      "sentiScore                         384 non-null int64\n",
      "stemmed                            0 non-null float64\n",
      "fee                                0 non-null float64\n",
      "title                              0 non-null float64\n",
      "dataSource                         384 non-null object\n",
      "dtypes: float64(3), int64(11), object(8)\n",
      "memory usage: 69.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Rename add/remove to match bug_tt.json\n",
    "\n",
    "df.rename(columns={\"processed_text\":\"comment\",\"score\":\"rating\",\"userName\":\"reviewer\"}, inplace=True)\n",
    "\n",
    "df.drop(columns=[\"appTitle\",\"text\",\"fileDate\",\"fileCategories\",\"contentRating\"], inplace=True)\n",
    "\n",
    "df['stemmed'] = np.nan\n",
    "df['fee'] = np.nan\n",
    "df['title'] = np.nan\n",
    "df['dataSource'] = \"Data542_Dataset\"\n",
    "\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output to seperate json files based on the label\n",
    "### (labels are: \"Bug\",\"Feature\",\"UserExperience\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: UserExperience\n",
      "Number of UserExperience reviews available: 39\n",
      "Number of NOT_UserExperience reviews available: 345\n",
      "Selecting first 39 NOT_UserExperience reviews.\n",
      "\n",
      "Processing: Bug\n",
      "Number of Bug reviews available: 58\n",
      "Number of NOT_Bug reviews available: 326\n",
      "Selecting first 58 NOT_Bug reviews.\n",
      "\n",
      "Processing: Feature\n",
      "Number of Feature reviews available: 43\n",
      "Number of NOT_Feature reviews available: 341\n",
      "Selecting first 43 NOT_Feature reviews.\n",
      "\n",
      "Processing: Rating\n",
      "Number of Rating reviews available: 319\n",
      "Number of NOT_Rating reviews available: 65\n",
      "**Only 65 NOT_Rating reviews are available. Will use all of them and...\n",
      "Removing 254 Rating reviews to create a balanced data set.\n"
     ]
    }
   ],
   "source": [
    "#Output as .json\n",
    "\n",
    "colList = [\"label_UE\",\"label_BR\",\"label_FR\",\"label_R\"]\n",
    "labelNames = [\"UserExperience\",\"Bug\",\"Feature\",\"Rating\"]\n",
    "for n in range(0,len(colList)):\n",
    "    print(\"\\nProcessing:\",labelNames[n])\n",
    "    \n",
    "    #Make a temp dataframe with all of the reviews of the specific label\n",
    "    tempdf=df.drop(colList, axis=1)[df[colList[n]]==1]\n",
    "    tempdf[\"label\"] = labelNames[n]\n",
    "    cnt = len(tempdf) #how many reviews are there of the specific label\n",
    "    print(\"Number of\",labelNames[n],\"reviews available:\",cnt)\n",
    "    \n",
    "    #Make a second temp dataframe with all of the reviews that are NOT the specific label\n",
    "    notTempdf=df.drop(colList, axis=1)[df[colList[n]]==0]\n",
    "    notTempdf[\"label\"] = (\"Not_\"+labelNames[n])\n",
    "    #Select only as many as are needed to match the amount of reviews that are for the label\n",
    "    cnt2 = len(notTempdf)\n",
    "    print(\"Number of NOT_\"+labelNames[n],\"reviews available:\",cnt2)\n",
    "    if cnt2 >= cnt:\n",
    "        print(\"Selecting first\",cnt,\"NOT_\"+labelNames[n],\"reviews.\")\n",
    "        notTempdf=notTempdf.iloc[0:cnt].copy()\n",
    "    else:\n",
    "        print(\"**Only\", cnt2, \"NOT_\"+labelNames[n], \"reviews are available. Will use all of them and...\")\n",
    "        print(\"Removing\",cnt-cnt2,labelNames[n],\"reviews to create a balanced data set.\")\n",
    "        tempdf = tempdf.iloc[0:cnt2].copy()\n",
    "\n",
    "    tempdf = tempdf.append(notTempdf)\n",
    "    tempdf.to_json(labelNames[n]+\"_ourdata.json\", orient=\"records\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
